{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment analysis on reviews data\n",
    "Kei Sato\n",
    "\n",
    "ML310B - Advanced Machine Learning\n",
    "\n",
    "March 25, 2019\n",
    "\n",
    "#### Project overview\n",
    "For this assignment, we want to use sentiment analysis to predict the polarity of a given film review.  To build the model, we are given a corpus of 50K reviews, each associated with a score of 0 or 1, which respectively indicate that the review is negative or positive.  \n",
    "\n",
    "#### Metrics used\n",
    "We will use accuracy as the main metric used to determine if the model is successful.  But, throughout the model training and cross validation, the proportion of false positives for both classes will be monitored.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive and negative reviews \n",
      " 1    25000\n",
      "0    25000\n",
      "Name: sentiment, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>My family and I normally do not watch local mo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Believe it or not, this was at one time the wo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>After some internet surfing, I found the \"Home...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>One of the most unheralded great works of anim...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>It was the Sixties, and anyone with long hair ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  My family and I normally do not watch local mo...          1\n",
       "1  Believe it or not, this was at one time the wo...          0\n",
       "2  After some internet surfing, I found the \"Home...          0\n",
       "3  One of the most unheralded great works of anim...          1\n",
       "4  It was the Sixties, and anyone with long hair ...          0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the data...\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "data = pd.read_csv('resources/Reviews.csv')\n",
    "print(\"Number of positive and negative reviews\", '\\n', data[\"sentiment\"].value_counts())\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initial Text Processing\n",
    "The reviews corpus has 50,000 reviews and is evenly split between positive and negative reviews, so that it contains 25,000 positive and 25,000 negative reviews.  Before doing any more data exploration, we process the text using standard techniques.  Much of this code was taken from the Lesson 8 HW assignment.\n",
    "\n",
    "The first step is apply some basic text processing, it was done in the following order.\n",
    "1.  Remove proper nouns:  This was done by using the NLTK position tagging functionality to identify proper nouns.\n",
    "2.  Expand contractions\n",
    "3.  Convert all text to lowercase\n",
    "4.  Remove `<br />` characters, this was because the `<br />` HTML tag was present in many reviews.  This part of cleaning the text was specific to this corpus.\n",
    "5.  Remove symbols and punctuation\n",
    "6.  Remove stop words.  For this application, I also removed the words \"movie\" and \"film\" because they were occured very often throughout positive and negative reviews.\n",
    "\n",
    "After cleaning the text, lemmatization is applied.  I did try to apply stemming to the dataset, but that produced too many non words and so it has been omitted from the text processing steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done processing data\n"
     ]
    }
   ],
   "source": [
    "# Taken Lesson 8 HW assignment\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import json\n",
    "\n",
    "# setting global variables\n",
    "with open('resources/contractions.json', 'r') as f:\n",
    "    contractions = json.load(f)\n",
    "contractions_keys = contractions.keys()\n",
    "\n",
    "replace_re_by_space = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "delete_re_symbols = re.compile('[^0-9a-z #+_]')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.add(\"movie\")\n",
    "stop_words.add(\"film\")\n",
    "\n",
    "# count = 0    \n",
    "\n",
    "def combine_tokened_words(tokened_words):\n",
    "    length_of_string=len(tokened_words)\n",
    "    text_new=\"\"\n",
    "    for w in tokened_words:\n",
    "        if w!=tokened_words[length_of_string-1]:\n",
    "             text_new=text_new+w+\" \" # when w is not the last word so separate by whitespace\n",
    "        else:\n",
    "            text_new=text_new+w\n",
    "    return text_new\n",
    "\n",
    "# converts to lowercase and removes <br />, punctuation, stop words, and numbers\n",
    "def text_processing(text):\n",
    "#     global count\n",
    "#     count+=1\n",
    "#     if (count % 500 == 0):\n",
    "#         print(\"COUNT:\", count)\n",
    "\n",
    "    tagged_sentence = tag.pos_tag(text.split())\n",
    "    text = [word for word,tag in tagged_sentence if tag != 'NNP' and tag != 'NNPS']\n",
    "    test = list(map(lambda word: contractions[word] if word in contractions_keys else word, text))\n",
    "    text = (' '.join(text))\n",
    "    \n",
    "    text = text.lower()\n",
    "    text = text.replace(\"<br />\", '')\n",
    "    text = re.sub(replace_re_by_space.pattern, ' ', text)\n",
    "    text = re.sub(delete_re_symbols.pattern, '', text)\n",
    "    token_word = word_tokenize(text)\n",
    "\n",
    "    # filtered_sentence contain all words that are not in stopwords dictionary    \n",
    "    filtered_sentence = [w for w in token_word if not w in stop_words]\n",
    "    return filtered_sentence\n",
    "\n",
    "# Lemmatizes words\n",
    "def text_lemmatization(text):\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    text = list(map(lambda word: wordnet_lemmatizer.lemmatize(word), text))\n",
    "    return text\n",
    "\n",
    "# test_data = data[:500].copy(deep=True)\n",
    "test_data = data.copy(deep=True)\n",
    "test_data[\"review\"] = test_data[\"review\"].apply(lambda text:\n",
    "                                                combine_tokened_words(\n",
    "                                                    text_lemmatization(\n",
    "                                                        text_processing(text)\n",
    "                                                    )\n",
    "                                                  )\n",
    "                                               )\n",
    "print(\"done processing data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data exploration\n",
    "Below is some initial data exploration.  We can see that the average length of positive and negative reviews is roughtly the same.  The ten most frequently occuring words are also very similar across between the sets of positive and negative reviews.  I also outputted the ten least commonly occuring words, in part for my own curiosity and to verify that the ten least commonly occuring words were still complete words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average word count of negative reviews: 100\n",
      "Average word count of positive reviews: 99\n",
      "\n",
      "\n",
      "10 most common words in negative reviews: [('one', 23823), ('like', 21496), ('even', 14625), ('character', 13744), ('good', 13448), ('would', 13361), ('time', 13139), ('get', 12928), ('bad', 12683), ('make', 12312)]\n",
      "10 least common words in negative reviews: [('callinternet', 1), ('garbageanyone', 1), ('60ship', 1), ('twitter', 1), ('fauxinfant', 1), ('seachange', 1), ('lessoften', 1), ('yearsmany', 1), ('imaginationchallenged', 1), ('flopperoo', 1)]\n",
      "\n",
      "\n",
      "10 most common words in positive reviews: [('one', 25089), ('like', 16898), ('time', 14278), ('character', 13460), ('good', 13447), ('story', 12856), ('see', 11564), ('great', 11314), ('make', 10805), ('get', 10792)]\n",
      "10 least common words in positive reviews: [('ralphtheallpurposeanimal', 1), ('filmedhumanity', 1), ('nextarguebly', 1), ('breakem', 1), ('1010big', 1), ('samaddhi', 1), ('fourmillion', 1), ('ucomfort', 1), ('upoint', 1), ('ufast', 1)]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import Counter \n",
    "from functools import reduce\n",
    "from operator import itemgetter\n",
    "import heapq\n",
    "\n",
    "# Get average length of reviews\n",
    "def get_avg_length_review(data, sentiment):\n",
    "    relevant_reviews = data.loc[data[\"sentiment\"] == sentiment][\"review\"]\n",
    "    avg_review_length = list(map(lambda review: len(review.split()), relevant_reviews))\n",
    "    return int(np.mean(avg_review_length))\n",
    "print(\"Average word count of negative reviews:\", get_avg_length_review(test_data, 0))\n",
    "print(\"Average word count of positive reviews:\", get_avg_length_review(test_data, 1))\n",
    "\n",
    "# Get 10 most and least frequently occuring words, verify that real words are coming through\n",
    "def get_most_least_common_words(data, sentiment):\n",
    "    relevant_reviews = data.loc[data[\"sentiment\"] == sentiment][\"review\"]\n",
    "    all_relevant_reviews = reduce(lambda accum, curr: accum + curr, relevant_reviews)\n",
    "    counted_words = Counter(all_relevant_reviews.split())\n",
    "    most_common = counted_words.most_common(10)\n",
    "    least_common = heapq.nsmallest(10, counted_words.items(), key=itemgetter(1))\n",
    "    return most_common, least_common\n",
    "\n",
    "negative_reviews = get_most_least_common_words(test_data, 0)\n",
    "positive_reviews = get_most_least_common_words(test_data, 1)\n",
    "\n",
    "print('\\n')\n",
    "print(\"10 most common words in negative reviews:\", negative_reviews[0])\n",
    "print(\"10 least common words in negative reviews:\", negative_reviews[1])\n",
    "print('\\n')\n",
    "print(\"10 most common words in positive reviews:\", positive_reviews[0])\n",
    "print(\"10 least common words in positive reviews:\", positive_reviews[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Methodology\n",
    "The data transformed by using TFIDIF then fed into a Logistic Regression classifier.  I am using a test train split of 30% and 70%.  The model is run with data that is processed with different ngram lengths and minimum document frequency values.  I am also using the SKlearn cross validation module to test different parameters for the Logistic Regression model itself.\n",
    "\n",
    "I chose the Logistic Regression model because this is a supervised learning problem with binary labels.  I also tried running an SVM model but it took too long to run on my machine with the complete dataset.  Using a decision tree was also considered but the dataset seemed too sparse to use that model.\n",
    "\n",
    "For this example, the minimium document frequency is varied while the maximum document frequency remains constant at 0.9.  The model is tested against the following min_df values: 0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05.  I chose to vary the minimum value instead of the maximum value because I assumed that would reduce the instances of non words in the dataset as well as reducing noise.  However, I am interested in the effect that changing the max_df would have as well.  The n-grams value is also varied, using values (1, 1), (1, 2), and (1, 3).  I did not go beyond (1, 3) because using a smaller portion of data, I was not seeing a significant difference in results after categorizing words using n-grams greater than 3.  After processing this data, if the transformed TFIDIF matrix had more features than instances, I would not run the model.  This was done because when running the model on such a matrix (with a subset of data) there was performance degredation, which was likely due to the Logistic Regression classifier not handling datasets with more features than rows well.  \n",
    "- Reducing the feature spacing using the sklearn Truncated SVD module was also considered.  However, in the documentation, the suggested value for n_components is 100, and when I ran the model with that parameter set as 200, the accuracy remained flat.  I believe this is because the same 200 n-grams were being selected for each run.  Some variation of PCA is appropriate for this exercise, but because, I assume, the n_components parameter would need to be well over the recommended value of 100 to affect this dataset, that step has been forgone.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "    \n",
    "def get_incorrect_predictions(data, y_true, y_pred):\n",
    "    predicted_pos = 0\n",
    "    predicted_neg = 0\n",
    "    correct_predictions = 0\n",
    "    incorrect_predictions = pd.DataFrame({'review': [], 'sentiment': []})\n",
    "    for i in range(0, len(y_true)):\n",
    "        if y_true[i] == y_pred[i]:\n",
    "            correct_predictions+=1\n",
    "        else:\n",
    "            incorrect_predictions.loc[len(incorrect_predictions)] = [data[i], y_pred[i]]\n",
    "            if y_pred[i] == 1:\n",
    "                predicted_pos+=1\n",
    "            else:\n",
    "                predicted_neg+=1\n",
    "    print(\"Predicted POSITIVE, actually NEGATIVE\", round(float(predicted_pos)/float(len(y_true)), 3))\n",
    "    print(\"Predicted NEGATIVE, actually POSITIVE\", round(float(predicted_neg)/float(len(y_true)), 3))\n",
    "    print('\\n')\n",
    "    return incorrect_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# returns the accuracy\n",
    "def print_metrics(x_test, y_test, y_pred):\n",
    "    acc = round(accuracy_score(y_test, y_pred), 3)\n",
    "    print(\"Accuracy:\", acc)        \n",
    "    print(\"AUC Score:\", round(roc_auc_score(y_test, y_pred), 3))\n",
    "\n",
    "#     incorrect_pred = get_incorrect_predictions(list(x_test), list(y_test), y_pred)\n",
    "#     print(\"For incorrectly predicted reviews:\")\n",
    "#     print(\"Most and least common of predicted negative reviews:\", get_most_least_common_words(incorrect_pred, 0))\n",
    "#     print(\"Most and least common of predicted positive reviews:\", get_most_least_common_words(incorrect_pred, 1))\n",
    "    return acc\n",
    "\n",
    "def run_model_cv(data):   \n",
    "    x_train, x_test, y_train, y_test = train_test_split(\n",
    "        data[\"review\"],\n",
    "        data[\"sentiment\"],\n",
    "        test_size=0.3,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    \n",
    "    ngram_range = [\n",
    "        (1, 1),\n",
    "        (1, 2),\n",
    "        (2, 2),\n",
    "        (2, 3),\n",
    "        (2, 4),\n",
    "        (3, 4),\n",
    "        (3, 4)\n",
    "    ]\n",
    "    \n",
    "    min_df_range = [\n",
    "        0.00001,\n",
    "        0.000025,\n",
    "        0.00005,\n",
    "        0.0001,\n",
    "        0.00025,\n",
    "        0.0005,\n",
    "        0.001\n",
    "    ]\n",
    "    \n",
    "    accuracy_by_ngram = []\n",
    "    for ngram_param in ngram_range:\n",
    "        accuracy_by_min_df = []\n",
    "        for min_df_param in min_df_range:\n",
    "            print(\"ngram range and min_df value\", ngram_param, min_df_param)\n",
    "            vectorizer = TfidfVectorizer(max_df=0.9, min_df=min_df_param, ngram_range=ngram_param).fit(x_train)\n",
    "            _x_train = vectorizer.transform(x_train)\n",
    "\n",
    "            # If there are more columns than rows, exit           \n",
    "            if (_x_train.shape[1] < _x_train.shape[0]):\n",
    "                _x_test = vectorizer.transform(x_test)\n",
    "#                 print(\"done vectorizing\")\n",
    "\n",
    "                cv_clf = GridSearchCV(\n",
    "                            LogisticRegression(),\n",
    "                            [\n",
    "                                {\n",
    "                                    \"solver\": [\"sag\", \"saga\"],\n",
    "                                    \"C\": [0.7, 0.9, 0.95, 0.975, 0.99],\n",
    "                                    \"max_iter\": [150, 200, 250]\n",
    "                                }\n",
    "                            ],\n",
    "                            cv=5,\n",
    "                            refit=True\n",
    "                        )\n",
    "                print(\"fitting the model on dataset with these dimensions\", _x_train.shape)\n",
    "                cv_clf.fit(_x_train, y_train)\n",
    "                print(\"Best params:\", cv_clf.best_params_)\n",
    "\n",
    "                y_pred = cv_clf.predict(_x_test)\n",
    "                accuracy = print_metrics(x_test, y_test, y_pred)\n",
    "                accuracy_by_min_df.append(accuracy)\n",
    "            else:\n",
    "                accuracy_by_min_df.append(0)\n",
    "                print(\"Too many features, exiting\", _x_train.shape)\n",
    "        accuracy_by_ngram.append({\n",
    "          \"scores\": accuracy_by_min_df,\n",
    "          \"label\": ngram_param\n",
    "        })\n",
    "    return accuracy_by_ngram, min_df_range\n",
    "\n",
    "model_results = run_model_cv(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization\n",
    "The resulting graph displays the minimum document frequency against the accuracy, coded by the n-gram value. \n",
    "The accuracy seems unaffected by the n-gram length and goes down when increasing minimum document frequency.  In general, most parameters yield an accuracy between 85% and 89%, there is not a significant difference between the proportion of misclassified positive or negative documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-8e4abea21c5b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mgraph_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model_results' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def graph_accuracy(accuracies, min_dfs):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.gca().set_ylim(bottom=0.8)\n",
    "    for accuracy in accuracies:\n",
    "        plt.scatter(min_dfs, list(accuracy[\"scores\"]), label=accuracy[\"label\"])\n",
    "        plt.xlabel('Minimum document frequency')\n",
    "        plt.ylabel('Accuracy')\n",
    "    plt.legend(bbox_to_anchor=(0, -0.15, 1, 0), loc=\"best\", borderaxespad=0, title=\"n gram\")\n",
    "    plt.show()\n",
    "\n",
    "graph_accuracy(model_results[0], model_results[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis\n",
    "The results across using different values for n-grams and minimum document frequencies are fairly similar.  Most successful runs of the model yielded accuracies between 85% and 89%.  The model was often not run when the data processing transformed the input so that the data had more features than data observations.  This was in part because the high dimensionality would interfere with the model performance, but also when the model was run with such data, there was not a significant improvement in performance (the accuracy was never greater than 90%).  \n",
    "\n",
    "The model also consistently misclassified positive and negative reviews at roughly the same rate.  \n",
    "\n",
    "The greatest accuracy achieved was roughly 88.7%.  This was achieved using n-grams of 1 to 3 and 1 to 4 with a minimum document frequency of 0.0005.  However a similar level of accuracy of 88.4% was also achieved using an n-gram of 1, with the same value of document frequency.  This may be in part because the data produced using a 1-3 n-gram and 1-4 n-gram value includes the unigram dataset as well.  Both Logistic Regression models had the highest value for a C parameter, of .99.  This likely indicates that the cross validation chose C to avoid overfitting to the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion\n",
    "For a Logistic Regression classifier on a dataset of this size, the optimal n-gram to apply is 1-3 or 1-4, while eliminating items that appear in less than 0.0005 documents.\n",
    "\n",
    "The main challenge with this dataset is its high dimensionality.  While there are 50,000 data points, there are many more features, in this case n-grams, within this corpus.  Cleaning the data can involve many word transformations, as well detecting if a word is mispelled.  Even after the data is cleaned, we have to determine how much the data to use with the model to reduce the large amount of noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Further analysis\n",
    "While this model was able to achieve a roughly 90% accuracy across both classes, there are many more ways to improve on this, either by doing more feature engineering or using a more robust model.\n",
    "\n",
    "Different models:  It would be worthwhile to try different models on the dataset.  An SVM would be appropriate because this is a binary classification problem.  However, I am also interested in the effects of measuring document similarity and using that for a clustering model.\n",
    "\n",
    "Word attributes:  There are other qualities of the individual words that could be further processed.  For example, whether or not the average word length of a review is correlated to the sentiment of the review.  Other aspects could include the obscurity and if the words are mispelled.\n",
    "\n",
    "N-grams and phrases:  This implmentation is processing the data into different n-grams, but there could be more analysis done with n-grams.  Such as collecting longer n-grams (3 to 4 words) by sentiment of the review they appear in and weighting those phrases more while training data.\n",
    "\n",
    "Stemming:  I chose not to include stemming because the nltk libraries were producing too many non words, but it would definitely be worthwhile to invest more time into applying stemming correctly.\n",
    "\n",
    "Sentence structure:  The sentence structure used could be indicative of the document's sentiment.  One hypothesis is that negative reviews have more sentences written in the first person, such beginning with \"I think ...\"  We can also explore if positive or negative reviews are correlated with incorrectly or correctly structured sentences.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### References:\n",
    "NLTK book http://www.nltk.org/book/\n",
    "\n",
    "Blog post on sentiment analysis https://towardsdatascience.com/understanding-feature-engineering-part-3-traditional-methods-for-text-data-f6f7d70acd41"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
