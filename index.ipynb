{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment analysis on reviews data\n",
    "Kei Sato\n",
    "\n",
    "ML310B - Advanced Machine Learning\n",
    "\n",
    "March 25, 2019\n",
    "\n",
    "\n",
    "We will be using reviews data to develop a sentiment analyzer, such that given a document, the model can predict if the review is positive (sentiment = 1) or negative (sentiment = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive and negative reviews \n",
      " 1    25000\n",
      "0    25000\n",
      "Name: sentiment, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>My family and I normally do not watch local mo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Believe it or not, this was at one time the wo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>After some internet surfing, I found the \"Home...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>One of the most unheralded great works of anim...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>It was the Sixties, and anyone with long hair ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  My family and I normally do not watch local mo...          1\n",
       "1  Believe it or not, this was at one time the wo...          0\n",
       "2  After some internet surfing, I found the \"Home...          0\n",
       "3  One of the most unheralded great works of anim...          1\n",
       "4  It was the Sixties, and anyone with long hair ...          0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the data...\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "data = pd.read_csv('resources/Reviews.csv')\n",
    "\n",
    "print(\"Number of positive and negative reviews\", '\\n', data[\"sentiment\"].value_counts())\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initial Text Processing\n",
    "The reviews corpus has 50,000 reviews and is evenly split between positive and negative reviews, so that it contains 25,000 positive and 25,000 negative reviews.  Before doing any more data exploration, we process the text using standard techniques.  Much of this code was taken from the Lesson 8 HW assignment.\n",
    "\n",
    "The first step is apply some basic text processing, it was done in the following order.\n",
    "1.  Remove proper nouns:  This was done by using the NLTK position tagging functionality to identify proper nouns.\n",
    "2.  Expand contractions\n",
    "3.  Convert all text to lowercase\n",
    "4.  Remove `<br />` characters, this was because the `<br />` HTML tag was present in many reviews.  This part of cleaning the text was specific to this corpus.\n",
    "5.  Remove symbols and punctuation\n",
    "6.  Remove stop words.  For this application, I also removed the words \"movie\" and \"film\" because they were occured very often throughout positive and negative reviews.\n",
    "\n",
    "After cleaning the text, lemmatization is applied.  I did try to apply stemming to the dataset, but that produced too many non words and so it has been omitted from the text processing steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COUNT: 500\n",
      "COUNT: 1000\n",
      "COUNT: 1500\n",
      "COUNT: 2000\n",
      "COUNT: 2500\n",
      "COUNT: 3000\n",
      "COUNT: 3500\n",
      "COUNT: 4000\n",
      "COUNT: 4500\n",
      "COUNT: 5000\n",
      "COUNT: 5500\n",
      "COUNT: 6000\n",
      "COUNT: 6500\n",
      "COUNT: 7000\n",
      "COUNT: 7500\n",
      "COUNT: 8000\n",
      "COUNT: 8500\n",
      "COUNT: 9000\n",
      "COUNT: 9500\n",
      "COUNT: 10000\n",
      "COUNT: 10500\n",
      "COUNT: 11000\n",
      "COUNT: 11500\n",
      "COUNT: 12000\n",
      "COUNT: 12500\n",
      "COUNT: 13000\n",
      "COUNT: 13500\n",
      "COUNT: 14000\n",
      "COUNT: 14500\n",
      "COUNT: 15000\n",
      "COUNT: 15500\n",
      "COUNT: 16000\n",
      "COUNT: 16500\n",
      "COUNT: 17000\n",
      "COUNT: 17500\n",
      "COUNT: 18000\n",
      "COUNT: 18500\n",
      "COUNT: 19000\n",
      "COUNT: 19500\n",
      "COUNT: 20000\n",
      "COUNT: 20500\n",
      "COUNT: 21000\n",
      "COUNT: 21500\n",
      "COUNT: 22000\n",
      "COUNT: 22500\n",
      "COUNT: 23000\n",
      "COUNT: 23500\n",
      "COUNT: 24000\n",
      "COUNT: 24500\n",
      "COUNT: 25000\n",
      "COUNT: 25500\n",
      "COUNT: 26000\n",
      "COUNT: 26500\n",
      "COUNT: 27000\n",
      "COUNT: 27500\n",
      "COUNT: 28000\n",
      "COUNT: 28500\n",
      "COUNT: 29000\n",
      "COUNT: 29500\n",
      "COUNT: 30000\n",
      "COUNT: 30500\n",
      "COUNT: 31000\n",
      "COUNT: 31500\n",
      "COUNT: 32000\n",
      "COUNT: 32500\n",
      "COUNT: 33000\n",
      "COUNT: 33500\n",
      "COUNT: 34000\n",
      "COUNT: 34500\n",
      "COUNT: 35000\n",
      "COUNT: 35500\n",
      "COUNT: 36000\n",
      "COUNT: 36500\n",
      "COUNT: 37000\n",
      "COUNT: 37500\n",
      "COUNT: 38000\n",
      "COUNT: 38500\n",
      "COUNT: 39000\n",
      "COUNT: 39500\n",
      "COUNT: 40000\n",
      "COUNT: 40500\n",
      "COUNT: 41000\n",
      "COUNT: 41500\n",
      "COUNT: 42000\n",
      "COUNT: 42500\n",
      "COUNT: 43000\n",
      "COUNT: 43500\n",
      "COUNT: 44000\n",
      "COUNT: 44500\n",
      "COUNT: 45000\n",
      "COUNT: 45500\n",
      "COUNT: 46000\n",
      "COUNT: 46500\n",
      "COUNT: 47000\n",
      "COUNT: 47500\n",
      "COUNT: 48000\n",
      "COUNT: 48500\n",
      "COUNT: 49000\n",
      "COUNT: 49500\n",
      "COUNT: 50000\n",
      "done processing data\n"
     ]
    }
   ],
   "source": [
    "# Taken Lesson 8 HW assignment\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import json\n",
    "\n",
    "# setting global variables\n",
    "with open('resources/contractions.json', 'r') as f:\n",
    "    contractions = json.load(f)\n",
    "contractions_keys = contractions.keys()\n",
    "\n",
    "replace_re_by_space = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "delete_re_symbols = re.compile('[^0-9a-z #+_]')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.add(\"movie\")\n",
    "stop_words.add(\"film\")\n",
    "\n",
    "count = 0    \n",
    "\n",
    "def combine_tokened_words(tokened_words):\n",
    "    length_of_string=len(tokened_words)\n",
    "    text_new=\"\"\n",
    "    for w in tokened_words:\n",
    "        if w!=tokened_words[length_of_string-1]:\n",
    "             text_new=text_new+w+\" \" # when w is not the last word so separate by whitespace\n",
    "        else:\n",
    "            text_new=text_new+w\n",
    "    return text_new\n",
    "\n",
    "# converts to lowercase and removes <br />, punctuation, stop words, and numbers\n",
    "def text_processing(text):\n",
    "    global count\n",
    "    count+=1\n",
    "    if (count % 500 == 0):\n",
    "        print(\"COUNT:\", count)\n",
    "\n",
    "    tagged_sentence = tag.pos_tag(text.split())\n",
    "    text = [word for word,tag in tagged_sentence if tag != 'NNP' and tag != 'NNPS']\n",
    "    test = list(map(lambda word: contractions[word] if word in contractions_keys else word, text))\n",
    "    text = (' '.join(text))\n",
    "    \n",
    "    text = text.lower()\n",
    "    text = text.replace(\"<br />\", '')\n",
    "    text = re.sub(replace_re_by_space.pattern, ' ', text)\n",
    "    text = re.sub(delete_re_symbols.pattern, '', text)\n",
    "    token_word = word_tokenize(text)\n",
    "\n",
    "    # filtered_sentence contain all words that are not in stopwords dictionary    \n",
    "    filtered_sentence = [w for w in token_word if not w in stop_words]\n",
    "    return filtered_sentence\n",
    "\n",
    "# Lemmatizes words\n",
    "def text_lemmatization(text):\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    text = list(map(lambda word: wordnet_lemmatizer.lemmatize(word), text))\n",
    "    return text\n",
    "\n",
    "# test_data = data[:10000].copy(deep=True)\n",
    "test_data = data.copy(deep=True)\n",
    "test_data[\"review\"] = test_data[\"review\"].apply(lambda text:\n",
    "                                                combine_tokened_words(\n",
    "                                                    text_lemmatization(\n",
    "                                                        text_processing(text)\n",
    "                                                    )\n",
    "                                                  )\n",
    "                                               )\n",
    "print(\"done processing data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data exploration\n",
    "Below is some initial data exploration.  We can see that the average length of positive and negative reviews is roughtly the same.  The ten most frequently occuring words are also very similar across between the sets of positive and negative reviews.  I also outputted the ten least commonly occuring words, in part for my own curiosity and to verify that the ten least commonly occuring words were still complete words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_avg_length_review\n",
      "Average word count of negative reviews: 100\n",
      "get_avg_length_review\n",
      "Average word count of positive reviews: 99\n",
      "get_most_least_common_words\n",
      "get_most_least_common_words\n",
      "\n",
      "\n",
      "10 most common words in negative reviews: [('one', 23823), ('like', 21496), ('even', 14625), ('character', 13744), ('good', 13448), ('would', 13361), ('time', 13139), ('get', 12928), ('bad', 12683), ('make', 12312)]\n",
      "10 least common words in negative reviews: [('callinternet', 1), ('garbageanyone', 1), ('60ship', 1), ('twitter', 1), ('fauxinfant', 1), ('seachange', 1), ('lessoften', 1), ('yearsmany', 1), ('imaginationchallenged', 1), ('flopperoo', 1)]\n",
      "\n",
      "\n",
      "10 most common words in positive reviews: [('one', 25089), ('like', 16898), ('time', 14278), ('character', 13460), ('good', 13447), ('story', 12856), ('see', 11564), ('great', 11314), ('make', 10805), ('get', 10792)]\n",
      "10 least common words in positive reviews: [('ralphtheallpurposeanimal', 1), ('filmedhumanity', 1), ('nextarguebly', 1), ('breakem', 1), ('1010big', 1), ('samaddhi', 1), ('fourmillion', 1), ('ucomfort', 1), ('upoint', 1), ('ufast', 1)]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import Counter \n",
    "from functools import reduce\n",
    "from operator import itemgetter\n",
    "import heapq\n",
    "\n",
    "# Get average length of reviews\n",
    "def get_avg_length_review(data, sentiment):\n",
    "    print(\"get_avg_length_review\")\n",
    "    relevant_reviews = data.loc[data[\"sentiment\"] == sentiment][\"review\"]\n",
    "    avg_review_length = list(map(lambda review: len(review.split()), relevant_reviews))\n",
    "    return int(np.mean(avg_review_length))\n",
    "print(\"Average word count of negative reviews:\", get_avg_length_review(test_data, 0))\n",
    "print(\"Average word count of positive reviews:\", get_avg_length_review(test_data, 1))\n",
    "\n",
    "# Get 10 most and least frequently occuring words, verify that real words are coming through\n",
    "def get_most_least_common_words(data, sentiment):\n",
    "    print(\"get_most_least_common_words\")\n",
    "    relevant_reviews = data.loc[data[\"sentiment\"] == sentiment][\"review\"]\n",
    "    all_relevant_reviews = reduce(lambda accum, curr: accum + curr, relevant_reviews)\n",
    "    counted_words = Counter(all_relevant_reviews.split())\n",
    "    most_common = counted_words.most_common(10)\n",
    "    least_common = heapq.nsmallest(10, counted_words.items(), key=itemgetter(1))\n",
    "    return most_common, least_common\n",
    "\n",
    "negative_reviews = get_most_least_common_words(test_data, 0)\n",
    "positive_reviews = get_most_least_common_words(test_data, 1)\n",
    "\n",
    "print('\\n')\n",
    "print(\"10 most common words in negative reviews:\", negative_reviews[0])\n",
    "print(\"10 least common words in negative reviews:\", negative_reviews[1])\n",
    "print('\\n')\n",
    "print(\"10 most common words in positive reviews:\", positive_reviews[0])\n",
    "print(\"10 least common words in positive reviews:\", positive_reviews[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running the model and cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "    \n",
    "def get_incorrect_predictions(data, y_true, y_pred):\n",
    "    predicted_pos = 0\n",
    "    predicted_neg = 0\n",
    "    correct_predictions = 0\n",
    "    incorrect_predictions = pd.DataFrame({'review': [], 'sentiment': []})\n",
    "    for i in range(0, len(y_true)):\n",
    "        if y_true[i] == y_pred[i]:\n",
    "            correct_predictions+=1\n",
    "        else:\n",
    "            incorrect_predictions.loc[len(incorrect_predictions)] = [data[i], y_pred[i]]\n",
    "            if y_pred[i] == 1:\n",
    "                predicted_pos+=1\n",
    "            else:\n",
    "                predicted_neg+=1\n",
    "    print(\"Predicted POSITIVE, actually NEGATIVE\", round(float(predicted_pos)/float(len(y_true)), 3))\n",
    "    print(\"Predicted NEGATIVE, actually POSITIVE\", round(float(predicted_neg)/float(len(y_true)), 3))\n",
    "    print('\\n')\n",
    "    return incorrect_predictions\n",
    "\n",
    "def graph_accuracy(accuracies, min_dfs):\n",
    "    for accuracy in accuracies:\n",
    "        plt.plot(min_dfs, list(accuracy[\"scores\"]), label=accuracy[\"label\"])\n",
    "        plt.xlabel('Minimum document frequency')\n",
    "        plt.ylabel('Accuracy')\n",
    "    plt.legend(bbox_to_anchor=(0, -0.15, 1, 0), loc=\"best\", borderaxespad=0, title=\"n gram\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ngram range (1, 1)\n",
      "min_df value 0.0001\n",
      "done vectorizing\n",
      "fitting the model on dataset with these dimensions (35000, 26535)\n",
      "Best params: {'C': 0.9, 'max_iter': 150, 'solver': 'sag'}\n",
      "Accuracy: 0.883\n",
      "AUC Score: 0.883\n",
      "Predicted POSITIVE, actually NEGATIVE 0.064\n",
      "Predicted NEGATIVE, actually POSITIVE 0.053\n",
      "\n",
      "\n",
      "For incorrectly predicted reviews:\n",
      "get_most_least_common_words\n",
      "Most and least common of predicted negative reviews: ([('one', 703), ('like', 620), ('good', 438), ('get', 409), ('would', 388), ('much', 384), ('time', 381), ('make', 379), ('really', 367), ('even', 353)], [('hater', 1), ('mdixon', 1), ('thermodynamics', 1), ('perished', 1), ('ecology', 1), ('verify', 1), ('burnzero', 1), ('madethe', 1), ('bullying', 1), ('filmso', 1)])\n",
      "get_most_least_common_words\n",
      "Most and least common of predicted positive reviews: ([('one', 842), ('like', 646), ('good', 477), ('story', 474), ('time', 443), ('see', 434), ('get', 422), ('character', 419), ('would', 412), ('really', 396)], [('tribal', 1), ('cheapest', 1), ('unscariest', 1), ('seductively', 1), ('tornapart', 1), ('badlyacted', 1), ('remiss', 1), ('exmilitary', 1), ('ducking', 1), ('endangered', 1)])\n",
      "\n",
      "ngram range (1, 1)\n",
      "min_df value 0.0005\n",
      "done vectorizing\n",
      "fitting the model on dataset with these dimensions (35000, 11404)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# returns the accuracy\n",
    "def print_metrics(x_test, y_test, y_pred):\n",
    "    acc = round(accuracy_score(y_test, y_pred), 3)\n",
    "    print(\"Accuracy:\", acc)        \n",
    "    print(\"AUC Score:\", round(roc_auc_score(y_test, y_pred), 3))\n",
    "\n",
    "    incorrect_pred = get_incorrect_predictions(list(x_test), list(y_test), y_pred)\n",
    "    print(\"For incorrectly predicted reviews:\")\n",
    "    print(\"Most and least common of predicted negative reviews:\", get_most_least_common_words(incorrect_pred, 0))\n",
    "    print(\"Most and least common of predicted positive reviews:\", get_most_least_common_words(incorrect_pred, 1))\n",
    "    return acc\n",
    "\n",
    "def run_model_cv(data):   \n",
    "    x_train, x_test, y_train, y_test = train_test_split(\n",
    "        data[\"review\"],\n",
    "        data[\"sentiment\"],\n",
    "        test_size=0.3,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    ngram_range = [\n",
    "        (1, 1),\n",
    "        (1, 2),\n",
    "        (1, 3)\n",
    "    ]\n",
    "    \n",
    "    min_df_range = [\n",
    "        0.0001,\n",
    "        0.0005,\n",
    "        0.001,\n",
    "        0.005,\n",
    "        0.01,\n",
    "        0.05\n",
    "    ]\n",
    "    \n",
    "    accuracy_by_ngram = []\n",
    "    for ngram_param in ngram_range:\n",
    "        accuracy_by_min_df = []\n",
    "        for min_df_param in min_df_range:\n",
    "            print(\"\\nngram range\", ngram_param)\n",
    "            print(\"min_df value\", min_df_param)\n",
    "            vectorizer = TfidfVectorizer(max_df=0.9, min_df=min_df_param, ngram_range=ngram_param).fit(x_train)\n",
    "            _x_train = vectorizer.transform(x_train)\n",
    "            \n",
    "            # If there are more columns than rows, exit           \n",
    "            if (_x_train.shape[1] < _x_train.shape[0]):\n",
    "                _x_test = vectorizer.transform(x_test)\n",
    "                print(\"done vectorizing\")\n",
    "\n",
    "                cv_clf = GridSearchCV(\n",
    "                            LogisticRegression(),\n",
    "                            [\n",
    "                                {\n",
    "                                    \"solver\": [\"sag\", \"saga\"],\n",
    "                                    \"C\": [0.1, 0.5, 0.9],\n",
    "                                    \"max_iter\": [150, 200, 250]\n",
    "                                }\n",
    "                            ],\n",
    "                            cv=5,\n",
    "                            refit=True\n",
    "                        )\n",
    "                print(\"fitting the model on dataset with these dimensions\", _x_train.shape)\n",
    "                cv_clf.fit(_x_train, y_train)\n",
    "                print(\"Best params:\", cv_clf.best_params_)\n",
    "\n",
    "                y_pred = cv_clf.predict(_x_test)\n",
    "                accuracy = print_metrics(x_test, y_test, y_pred)\n",
    "                accuracy_by_min_df.append(accuracy)\n",
    "            else:\n",
    "                accuracy_by_min_df.append(0)\n",
    "                print(\"Too many features, exiting\", _x_train.shape)\n",
    "        accuracy_by_ngram.append({\n",
    "          \"scores\": accuracy_by_min_df,\n",
    "          \"label\": ngram_param\n",
    "        })\n",
    "    print(\"TRYING TO PLOT\")\n",
    "    print(accuracy_by_ngram)\n",
    "    print(min_df_range)\n",
    "    if (len(accuracy_by_ngram) != 0):\n",
    "        graph_accuracy(accuracy_by_ngram, min_df_range)\n",
    "\n",
    "run_model_cv(test_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
