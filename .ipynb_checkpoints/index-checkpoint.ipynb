{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment analysis on reviews data\n",
    "Kei Sato\n",
    "\n",
    "ML310B - Advanced Machine Learning\n",
    "\n",
    "March 25, 2019\n",
    "\n",
    "\n",
    "We will be using reviews data to develop a sentiment analyzer, such that given a document, the model can predict if the review is positive (sentiment = 1) or negative (sentiment = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive and negative review \n",
      " 1    25000\n",
      "0    25000\n",
      "Name: sentiment, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>My family and I normally do not watch local mo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Believe it or not, this was at one time the wo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>After some internet surfing, I found the \"Home...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>One of the most unheralded great works of anim...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>It was the Sixties, and anyone with long hair ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  My family and I normally do not watch local mo...          1\n",
       "1  Believe it or not, this was at one time the wo...          0\n",
       "2  After some internet surfing, I found the \"Home...          0\n",
       "3  One of the most unheralded great works of anim...          1\n",
       "4  It was the Sixties, and anyone with long hair ...          0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the data...\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "data = pd.read_csv('data/Reviews.csv')\n",
    "\n",
    "print(\"Number of positive and negative review\", '\\n', data[\"sentiment\"].value_counts())\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initial Text Processing\n",
    "The reviews corpus has 5000 reviews and is evenly split between positive and negative reviews, so that it contains 2500 positive and 2500 negative reviews.  Before doing any more data exploration, we process the text using standard techniques.  Much of this code was taken from the Lesson 8 HW assignment.\n",
    "\n",
    "The first step is apply some basic text processing.  This function will transform all the letters to lowercase and replace any punctuation or symbols with spaces.  At this step we will also remove English stop words.  Because this corpus contains some <br \\> HTML elements, we will strip those out from the text as well.  This function will return the words in a tokenized format such each word is an element in an array.  After cleaning the text, lemmatization is applied. \n",
    "\n",
    "I did apply stemming to the dataset, but that produced too many non words and so it has been omitted from the text processing steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taken Lesson 8 HW assignment\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "replace_re_by_space = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "delete_re_symbols = re.compile('[^0-9a-z #+_]')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def combine_tokened_words(tokened_words):\n",
    "    length_of_string=len(tokened_words)\n",
    "    text_new=\"\"\n",
    "    for w in tokened_words:\n",
    "        if w!=tokened_words[length_of_string-1]:\n",
    "             text_new=text_new+w+\" \" # when w is not the last word so separate by whitespace\n",
    "        else:\n",
    "            text_new=text_new+w\n",
    "    return text_new\n",
    "\n",
    "# converts to lowercase and removes <br />, punctuation, stop words, and numbers\n",
    "def text_processing(text):\n",
    "    text = text.lower()\n",
    "    text = text.replace(\"<br />\", '')\n",
    "    text = re.sub(replace_re_by_space.pattern, ' ', text)\n",
    "    text = re.sub(delete_re_symbols.pattern, '', text)\n",
    "    token_word = word_tokenize(text)\n",
    "    \n",
    "    # filtered_sentence contain all words that are not in stopwords dictionary\n",
    "    filtered_sentence = [w for w in token_word if not w in stop_words]\n",
    "    return filtered_sentence\n",
    "\n",
    "# Lemmatizes words\n",
    "def text_lemmatization(text):\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    text = list(map(lambda word: wordnet_lemmatizer.lemmatize(word), text))\n",
    "    return text\n",
    "\n",
    "test_data = data[:500].copy(deep=True)\n",
    "test_data[\"review\"] = test_data[\"review\"].apply(lambda text:\n",
    "                                                combine_tokened_words(\n",
    "                                                text_lemmatization(\n",
    "                                                text_processing(text))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data exploration\n",
    "Below is some initial data exploration.  We can see that the average length of positive and negative reviews is roughtly the same.  The ten most frequently occuring words are also very similar across between the sets of positive and negative reviews.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average word count of negative reviews: 137\n",
      "Average word count of positive reviews: 122\n",
      "\n",
      "\n",
      "Top 10 most common words in negative reviews [('movie', 627), ('film', 427), ('one', 258), ('like', 237), ('get', 177), ('even', 159), ('character', 158), ('good', 156), ('would', 153), ('time', 137)]\n",
      "Top 10 most common words in positive reviews [('movie', 409), ('film', 407), ('one', 268), ('like', 173), ('time', 150), ('character', 140), ('see', 139), ('story', 130), ('well', 122), ('good', 119)]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import Counter \n",
    "from functools import reduce\n",
    "\n",
    "# Get average length of reviews\n",
    "def get_avg_length_review(data, sentiment):\n",
    "    relevant_reviews = data.loc[data[\"sentiment\"] == sentiment][\"review\"]\n",
    "    avg_review_length = list(map(lambda review: len(review.split()), relevant_reviews))\n",
    "    return int(np.mean(avg_review_length))\n",
    "print(\"Average word count of negative reviews:\", get_avg_length_review(test_data, 0))\n",
    "print(\"Average word count of positive reviews:\", get_avg_length_review(test_data, 1))\n",
    "\n",
    "# Get 10 most frequently occuring words\n",
    "def get_top_words(data, sentiment):\n",
    "    relevant_reviews = data.loc[data[\"sentiment\"] == sentiment][\"review\"]\n",
    "    all_relevant_reviews = reduce(lambda accum, curr: accum + curr, relevant_reviews)\n",
    "    return Counter(all_relevant_reviews.split()).most_common(10)\n",
    "print('\\n')  \n",
    "print(\"Top 10 most common words in negative reviews\", get_top_words(test_data, 0))\n",
    "print(\"Top 10 most common words in positive reviews\", get_top_words(test_data, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running the model and cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done vectorizing\n",
      "done cross validating\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'GridSearchCV' object has no attribute 'best_params_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-188ffb24d13b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0mrun_model_cv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-188ffb24d13b>\u001b[0m in \u001b[0;36mrun_model_cv\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     28\u001b[0m             )\n\u001b[1;32m     29\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"done cross validating\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Best params\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0mcv_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'GridSearchCV' object has no attribute 'best_params_'"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def run_model_cv(data):   \n",
    "    x_train, x_test, y_train, y_test = train_test_split(\n",
    "        data[\"review\"],\n",
    "        data[\"sentiment\"],\n",
    "        test_size=0.3,\n",
    "        random_state=42\n",
    "    )\n",
    "    tfid_vectorizer = TfidfVectorizer(min_df=5, max_df=0.9, use_idf=True).fit(x_train)\n",
    "    x_train = tfid_vectorizer.transform(x_train)\n",
    "    x_test = tfid_vectorizer.transform(x_test)\n",
    "    \n",
    "    print(\"done vectorizing\")\n",
    "    cv_clf = GridSearchCV(\n",
    "                LogisticRegression(),\n",
    "                [\n",
    "                    {\n",
    "                        \"solver\": [\"lbfgs\", \"sag\", \"saga\"],\n",
    "                        \"C\": [0.1, 0.5, 0.75, 0.9],\n",
    "                        \"class_weight\": [\"balanced\"]\n",
    "                    }\n",
    "                ],\n",
    "                cv=5,\n",
    "                refit=True\n",
    "            )\n",
    "    print(\"done cross validating\")\n",
    "    print(\"Best params\", cv_clf.best_params_)\n",
    "    cv_clf.fit(x_train, y_train)\n",
    "    score = cv_clf.score(x_test, y_test)\n",
    "    print(\"accuracy\", score)\n",
    "    \n",
    "    \n",
    "run_model_cv(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
